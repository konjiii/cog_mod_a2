{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2450dd1e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in bandit tasks and gridworlds\n",
    "\n",
    "Steven Miletic<br>\n",
    "<sub>Leiden University<sub><br><br>  \n",
    "    \n",
    "This is the code for the practical session in the course \"2425-S1 Cognitive Modelling: How to build a brain\" [(link to Brightspace)](https://brightspace.universiteitleiden.nl/d2l/home/309838)<br>   \n",
    "Date: September 30th 2024<br>\n",
    "\n",
    "Code based on [Franz Wurm's version of last year](https://github.com/fwurm/teaching_CognitiveModelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184f326",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253e22",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "### 1.1 Goal of this practical\n",
    "\n",
    "In this practical, you are going to implement a reinforcement learning agent for a standard decision-making problem as often used in psychological experiments. Our agent will be learn the value of states and actions in the environment simply by observing their associated rewards and punishments. As discussed in the lecture, this resembles the principles of classical and operant conditioning. It is a efficient way of learning that leads to simple, habitual behavioral patterns.\n",
    "\n",
    "Step by step, we will discuss the different necessary components to realize such a reinforcement learning agent.\n",
    "\n",
    "### 1.2 Setting up this notebook \n",
    "\n",
    "This notebook contains all necessary information for the first practical session. The exercise is intended to be completed chronologically, i.e. from top to bottom. It includes blocks of text with explanations and code blocks to execute commands and code, including code blocks where you need to add your code.\n",
    "\n",
    "A few recommendations:\n",
    "- I recommend to download the file and save it to a separate folder. Ideally, this folder is easily accessible (e.g. on your desktop) or implemented in a preexisting folder structure (e.g. MyBachelor>CogMod>PracticalRL).\n",
    "- I also recommend to work with copies. That means, don't work on the original file, but rather work on a copy. This makes sure that you do not delete important information and always have a basis to go back to. Additionally, you could implement version control, meaning you save your work to a new file from time to time (e.g., filename_v1, filename_v2). This makes sure you dont lose too much progress in case your computer shuts down or you forgot to press the save button.\n",
    "- For working on the excersises of the practical I suggest you use [Google Colab](https://colab.research.google.com/) or [Jupyter-notebook](https://jupyter.org/). While Google Colab is straight-foward to use, Jupyter-notebook might require you to install additional software (e.g., [Anaconda](https://anaconda.org/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1ebc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first, let's import some packages that we need\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89d82",
   "metadata": {},
   "source": [
    "# 2. A simple k-armed bandit task\n",
    "\n",
    "In the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), the agent is faced with a choice between $k$ multiple options. Named after slot machines in a casino, our one and two-armed versions in the lecture were used to demonstrate the two core principles that allow the agent to achieve his goal of reward maximzation.\n",
    "\n",
    "First, we'll simulate 'drawing' or 'sampling' from a bandit in a function -- i.e., we simulate pulling the lever of a slot machine. To simulate a slot machine, we need to specify how often the slot machine gives out a reward. That means that we need to specify the _reward distribution_, which is a probability distribution.\n",
    "\n",
    "The functions below uses a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). That is, on every draw, the bandit/slot machine either gives a reward, or not. The probability of reward is user-specified, and given by the argument <code>prob</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993fb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_from_one_bandit(n_trials, prob):\n",
    "    \n",
    "    reward_samples = np.random.binomial(1,prob,n_trials)  # this simulates the rewards\n",
    "    \n",
    "    return reward_samples\n",
    "\n",
    "def draw_from_k_bandits(n_trials, probs):\n",
    "    \n",
    "    n_bandits = len(probs)  # how many arms?\n",
    "    \n",
    "    # preallocate output\n",
    "    reward_samples = np.empty((n_bandits, n_trials))\n",
    "    reward_probs = np.empty((n_bandits, n_trials))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        reward_samples[i,:] = draw_from_one_bandit(n_trials, prob)\n",
    "        reward_probs[i,:] = prob\n",
    "        \n",
    "    return reward_samples, reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f44d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 2.1 Let's try it out! </h2><br>\n",
    "\n",
    "Generate two bandits with each 100 trials. The bandits should have reward probabilities of 80% and 20%, respectively.\n",
    "\n",
    "Verify that the simulated reward probabilities are indeed 80% and 20%. (hint: <code>np.mean()</code> can easily calcuate the mean)\n",
    "\n",
    "Try to write efficient code (few lines)!\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ff958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO: create bandits and print average reward\n",
    "\n",
    "n_trials = 100\n",
    "probs = [0.8, 0.2]\n",
    "\n",
    "reward_samples, reward_probs = draw_from_k_bandits(n_trials, probs)\n",
    "\n",
    "print(reward_samples)\n",
    "print(f'mean: {np.mean(reward_samples,axis=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b06a4-394d-442a-8f17-06b3e80240ce",
   "metadata": {},
   "source": [
    "### Learning and deciding\n",
    "As discussed in the lecture, the two principles of reinforcement learning are\n",
    "- the learning rule: _updating_ of expectations based on observations \n",
    "- the decision rule: taking _actions_ based on expectations\n",
    "\n",
    "Below, we'll first try out different learning rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e752eec",
   "metadata": {},
   "source": [
    "## 3. The learning rule\n",
    "\n",
    "The learning rule should help our agent to build up expectations about its environment.\n",
    "\n",
    "In this section, we are going to demonstrate why a _weighted-average_ method outperforms the _average_ method in changing environments.\n",
    "\n",
    "#### 3.1 Calculating the average\n",
    "\n",
    "As a first step, let us have a look at the performance of the averaging method for incremental learning in a stable environment.\n",
    "\n",
    "We specify the learning rule for incremental averaging in acordance with the formula introduced in the lecture.\n",
    "\n",
    "$V_{t+1} = V_{t} + \\frac{1} {t}(R_{t} - V_{t})$,\n",
    "\n",
    ">where $V$ is the estimated average value,<br>\n",
    ">and $t$ is the current time step.<br>\n",
    "\n",
    "The code below implements the incremental average method in a 'static' environment (i.e., the reward probabilities don't change across time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca64d04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "prob = [0.7]\n",
    "\n",
    "reward_samples, reward_probs = draw_from_k_bandits(n_trials,prob)\n",
    "\n",
    "mean_avg = np.empty((1,n_trials+1))   # Pre-allocate an array in which we'll store the running averages\n",
    "for iT in np.arange(n_trials):\n",
    "    if iT == 0:\n",
    "        mean_avg[0,0] = 0.5     ## Think about this for a moment: what are we doing here?\n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[0,iT]-mean_avg[0,iT])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5dc0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 3.2 Challenge </h2><br>\n",
    "\n",
    "Show that the incremental average mean indeed converges to the true reward probability.\n",
    "    \n",
    ">Use the <code>print</code> function and the matplotlib methods, loaded as <code>plt</code> to visualize the values obtained from the average method across trials\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b3762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6807f",
   "metadata": {},
   "source": [
    "#### 3.3 ...in changing environments\n",
    "\n",
    "So far, that looks great -- the average method of incremental learning does a good job in estimating the average on a step-by-step basis. However, what happens if the environment is not stable? By 'unstable' or 'changing' environments, we mean environments in which the reward probabilities change across trials.\n",
    "\n",
    "Below, the code simulates a changing environment by drawing 1000 samples from a bandit with 70% reward probability, followed by 1000 samples with 30% reward probability, followed by another 1000 trials with 70% reward probability, etc. The _average_ method is used to estimate the value of the bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06a86d-b4cc-46a3-9d66-96aa0e0c46e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "probs = [0.7, 0.3, 0.7, 0.3, 0.7, 0.3]\n",
    "\n",
    "reward_samples, reward_probs = draw_from_k_bandits(n_trials,probs)\n",
    "\n",
    "# Simulate a bandit with changing reward probabilities by\n",
    "# flattening a m-by-n matrix into a 1-by-m*n vector\n",
    "reward_samples = reward_samples.flatten()\n",
    "reward_probs = reward_probs.flatten()\n",
    "# this problem could also be solved by multiple loops\n",
    "\n",
    "mean_avg = np.empty((1,len(reward_probs)+1))\n",
    "for iT in np.arange(len(reward_probs)):    \n",
    "    if iT==0: #initialize at time point 0\n",
    "        mean_avg[0,0] = 0.5\n",
    "    #update estimated values    \n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[iT]-mean_avg[0,iT])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5126c-af53-4d8e-9128-450345af4207",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 3.4 Challenge </h2><br>\n",
    "\n",
    "Like you did in challenge 3.2, plot the estimated value obtained with the mean method over trials, and compare these with the true reward probabilities.\n",
    "\n",
    "Think about the results as displayed in the figure. How well does the average method capture the true reward probabilities? Why does it fail to do a very good job?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bf6f3-b5ef-40d7-b976-1b5768b3623b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a25f72-9aa9-43b0-9723-74c56f17f235",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the _weighted average_ method is often used as an improvement on the average method. The corresponding update rule is as follows:\n",
    "\n",
    "$V_{t+1} = V_{t} + \\alpha(R_{t} - V_{t})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429589a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 3.5 Challenge </h2><br>\n",
    "\n",
    "The code block below is a copy from the code block above, that simulates an unstable environment and uses the average method to learn the value.\n",
    "\n",
    "In the code block below, you need to add the estimates of values obatined using the _weighted average_ method. That is, implement the learning rule with the equation in the cell above. Assume a learning rate of 0.05.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a2110-c966-4f8f-b4e9-0d15dd28be96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add your code in this block\n",
    "\n",
    "n_trials = 1000\n",
    "probs = [0.7, 0.3, 0.7, 0.3, 0.7, 0.3]\n",
    "\n",
    "reward_samples, reward_probs = draw_from_k_bandits(n_trials,probs)\n",
    "\n",
    "# Simulate a bandit with changing reward probabilities by\n",
    "# flattening a m-by-n matrix into a 1-by-m*n vector\n",
    "reward_samples = reward_samples.flatten()\n",
    "reward_probs = reward_probs.flatten()\n",
    "# this problem could also be solved by multiple loops\n",
    "\n",
    "mean_avg = np.empty((1,len(reward_probs)+1))\n",
    "for iT in np.arange(len(reward_probs)):    \n",
    "    if iT==0: #initialize at time point 0\n",
    "        mean_avg[0,0] = 0.5\n",
    "    #update estimated values    \n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[iT]-mean_avg[0,iT])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f14da-f852-4d50-92b6-bf0c2f9637fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 3.6 Challenge </h2><br>\n",
    "\n",
    "Plot the results again. In your plot, include both the value estimates from the average and the weighted average method. Think about the results for a moment. Which method works better? And why?\n",
    "\n",
    "And what happens if you increase/decrease the learning rate?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91a650-5237-41b7-accb-61f4826140e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1fec",
   "metadata": {},
   "source": [
    "# 4. Decision rules: Policy\n",
    "\n",
    "So far, we have only dealt with prediction problems. However, the most powerful application of reinforcement learning lies in solving control problems. \n",
    "\n",
    "The decision rule (or policy) defines the agent's way of behaving at a given time or state. This is an extension of the previous prediction problem, as we now also consider actions given each state. Put differently, the policy is a mapping between states and actions and it corresponds to what psychologists sometimes call a stimulus-response association.\n",
    "\n",
    "\n",
    "### 4.1 Improving decision making... by exploration\n",
    "\n",
    "In order to replicate the results for the improvement of decision-making as shown in the lecture, we first need to set up a new bandit function.\n",
    "\n",
    "This new bandit function again takes two input parameters and gives two output variables. In contrast to the previous bandit, the current bandit will draw its reward from a normal (Gaussian) distribution. That means that, contrary to previous bandits that either gave a reward or gave no reward, the new bandits _always_ give a reward, but the amount of reward is continuous follows a Gaussian distribution.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 4.2 Challenge </h2><br>\n",
    "\n",
    "Add a function to the routine below that samples rewards from a normal distribution with a mean of <code>mean_rewards</code> and a standard deviation of <code>1</code>. <br>\n",
    "<br>\n",
    "    \n",
    ">HINT: Replace the <code>???</code> in the box below with the appropriate function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20c113-36f9-4012-b0a0-bf9629ff4e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_from_one_gaussian_bandit(n_trials, mean_reward):\n",
    "    \n",
    "    reward_samples = ???\n",
    "    \n",
    "    return reward_samples\n",
    "\n",
    "def draw_from_k_gaussian_bandits(n_trials, mean_rewards):\n",
    "    \n",
    "    n_bandits = len(mean_rewards)  # how many arms?\n",
    "    \n",
    "    # preallocate output\n",
    "    reward_samples = np.empty((n_bandits, n_trials))\n",
    "    reward_probs = np.empty((n_bandits, n_trials))\n",
    "    \n",
    "    for i, mean_reward in enumerate(mean_rewards):\n",
    "        reward_samples[i,:] = draw_from_one_gaussian_bandit(n_trials, mean_reward)\n",
    "        reward_means[i,:] = mean_reward\n",
    "        \n",
    "    return reward_samples, reward_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deacbcc",
   "metadata": {},
   "source": [
    "During the lecture, we introduced a few different decision rules and of course the list in not exhaustive. Below you find the most commonly used methods:\n",
    "- random method\n",
    "- greedy method\n",
    "- e-greedy method (nb: e is for epsilon)\n",
    "- softmax method\n",
    "\n",
    "Let's first implement the different methods for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199f99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def action_random(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    action = np.random.choice(actions)    # this literally just takes a (uniform distributed) random draw\n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_greedy(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    ties = np.array(values[:,0] == np.max(values[:,0]))     # here, we find all values that are maximum\n",
    "    action = np.random.choice(actions[ties])                # and among the maximum values, we choose 1 randomly\n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_epsilon(values,epsilon):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    if (np.random.uniform() < epsilon):\n",
    "        action = action_random(values)\n",
    "    else:\n",
    "        action = action_greedy(values)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a43ee",
   "metadata": {},
   "source": [
    "Let's make a few assumptions for our simulation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49175e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_agents = 100\n",
    "n_trials = 1000\n",
    "n_bandits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850cf33",
   "metadata": {},
   "source": [
    "### 4.3.1 Greedy agents\n",
    "\n",
    "In the following code block I have set up a number of greedy agents.\n",
    "\n",
    "In accordance with the lecture, both the average for the rewards (<code>rewards_greedy</code>) and the percentage of optimal decisions (<code>optimum_greedy</code>) will be saved for plotting.\n",
    "\n",
    "Please note, that we implement the _average methode_ as the learning rule. As this is a stable environment, the properties of the average method guarantee that our estimates converge to the true value (at least with an appropriate action selection method...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602050d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## greedy action selection\n",
    "\n",
    "rewards_greedy = np.empty((n_agents,n_trials))\n",
    "optimum_greedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents):\n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = draw_from_k_gaussian_bandits(n_trials, reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_greedy(values)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/iT)*PE\n",
    "\n",
    "        #store reward & optimal decision\n",
    "        rewards_greedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_greedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_greedy[iA,iT] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8082491",
   "metadata": {},
   "source": [
    "### 4.3.2 $\\epsilon$-greedy action selection\n",
    "\n",
    "Basically, the $\\epsilon$-greedy agent is identical to the greedy agent with the the minor detail that on 100*$\\epsilon$% of the trials a random action will be chosen.\n",
    "\n",
    "You find the code for $\\epsilon$-greedy actions selection already above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e231a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "rewards_egreedy = np.empty((n_agents,n_trials))\n",
    "optimum_egreedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents): \n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = draw_from_k_gaussian_bandits(n_trials,reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_epsilon(values,epsilon)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/iT)*PE\n",
    "        \n",
    "        #store reward\n",
    "        rewards_egreedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_egreedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_egreedy[iA,iT] = 0     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da293df-5ab0-437b-a420-fe063a9a1caf",
   "metadata": {},
   "source": [
    "Now let's plot the results and compare greedy vs epsilon-greedy action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f436c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_greedy,0),'-k',label='greedy')\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\") \n",
    "ax.set_xlabel('trial')\n",
    "ax.set_ylabel('average reward')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_greedy,0),'-k',label='greedy')\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\")   \n",
    "ax2.set_xlabel('trial')\n",
    "ax2.set_ylabel('% optimal choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389943db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 4.3.3 Challenge </h2><br>\n",
    "\n",
    "Huh, that's strange. These results do not seem to replicate the results from Sutton & Barto. In silico, the $\\epsilon$-greedy method should outperform the greedy method for this environment. \n",
    "However it does not seem to do so in our simulation.\n",
    "\n",
    "Can you figure out what's wrong with the code above? Hint: Have a look at the used updating function, and compare it with the correct updating function in the slides.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75905f1d",
   "metadata": {},
   "source": [
    "### 4.3.4 Softmax action selection\n",
    "\n",
    "Let's have a closer look at the softmax rule, as this is maybe the most widely used decision rule in the neuroscientific literature.\n",
    "\n",
    "$\\LARGE p(a)= \\frac{e ^{(\\beta * Q(a))}} {\\sum \\limits _{a'} e ^{(\\beta * Q(a'))}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a16300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def action_softm(values,beta):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    prob = ???\n",
    "    action = np.random.choice(actions,size = 1, p = prob) \n",
    "\n",
    "    return action, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f015c76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 4.3.5 Challenge </h2><br>\n",
    "\n",
    "The central part of the softmax algorithm is missing. Fill in the missing code.\n",
    "    \n",
    ">HINT: Use prespecified numpy methods.\n",
    "</div>\n",
    "\n",
    "If we want to put this softmax function to use, we can make a few hypothetical assumptions for our bandit task.\n",
    "\n",
    "For example, let's assume our agent has played the slot machine twice, each arm one time. The left arm lead to a reward (1), whereas the right arm did not result in a reward (0). We can translate this experience into simplified expectations (values) for the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a8dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_values = np.array([1,0])\n",
    "beta = 1\n",
    "\n",
    "action, prob = action_softm(dummy_values,beta)\n",
    "\n",
    "print('Chosen action: %d' % (action))\n",
    "print('Action probablities: %s' % (np.array2string(prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc554ab5",
   "metadata": {},
   "source": [
    "We can see, that the dummy expectations translate into action probabilites in a straightforward way. The highest value has the highest action probability.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 4.3.6 Idea </h2><br>\n",
    "Play around with the beta value and observe the changes!\n",
    "    \n",
    "</div>\n",
    "\n",
    "As already discussed in the lecture, the beta (or 'inverse temperature') affects the so-called [gain](https://en.wikipedia.org/wiki/Gain_(electronics)). The higher the gain, the more pronounced the differences in action values get translated into action probabilities.\n",
    "\n",
    "We can plot this in a systematic way.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> 4.3.7 Challenge </h2><br>\n",
    "Setup a plot similar to the slide in the lecture.<br>\n",
    "- x-axis: differences in values<br>\n",
    "- y-axis: action probablity for option 1<br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982f234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "betas = [0,1,2,5,10]\n",
    "x = np.arange(-2,2,0.1)\n",
    "\n",
    "all_probs = np.empty((len(betas),len(x)))\n",
    "\n",
    "## TODO: calculate action probabilities for each [beta,x] pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ec295",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Your code here: Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576fb78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h2> Assignments and more </h2><br>\n",
    "\n",
    "Congratulations! You have reached the end of the notebook. I hope you have learned something about reinforcement learning.\n",
    "\n",
    "Now it is time for your assignment. You find the assignments, additional material in the \"solution\" notebook.\n",
    "\n",
    "Feel free to discuss your own ideas for assignments with the workgroup teacher!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e088c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
